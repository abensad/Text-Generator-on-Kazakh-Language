{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lKeG6IdXTJja"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, Embedding\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 100\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "# dataset file path\n",
        "FILE_PATH = \"data/1-tom.txt\"\n",
        "BASENAME = os.path.basename(FILE_PATH)\n",
        "text = open(FILE_PATH, encoding=\"utf-8\").read()\n",
        "text = open(FILE_PATH, encoding=\"utf-8\").read()\n",
        "text = text.lower()\n",
        "text = text.translate(str.maketrans(\"\", \"\", punctuation))"
      ],
      "metadata": {
        "id": "dfEf5-ICTRrD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print some stats\n",
        "n_chars = len(text)\n",
        "vocab = ''.join(sorted(set(text)))\n",
        "print(\"unique_chars:\", vocab)\n",
        "n_unique_chars = len(vocab)\n",
        "print(\"Number of characters:\", n_chars)\n",
        "print(\"Number of unique characters:\", n_unique_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZAQUYb3X6c3",
        "outputId": "452f36e9-c930-46e7-c486-d8df19d785d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_chars: \n",
            " 01234567e«»абвгдежзийклмнопрстуфхчшщыьэюяіғқңүұһәө–﻿\n",
            "Number of characters: 717547\n",
            "Number of unique characters: 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary that converts characters to integers\n",
        "char2int = {c: i for i, c in enumerate(vocab)}\n",
        "# dictionary that converts integers to characters\n",
        "int2char = {i: c for i, c in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "3gx9jSpxYV5-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save these dictionaries for later generation\n",
        "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
        "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
      ],
      "metadata": {
        "id": "yz6DyNZvYgSj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all text into integers\n",
        "encoded_text = np.array([char2int[c] for c in text])\n",
        "# construct tf.data.Dataset object\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "# print first 5 characters\n",
        "for char in char_dataset.take(8):\n",
        "    print(char.numpy(), int2char[char.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPOfiDXFYh-G",
        "outputId": "db5a907b-9476-48c3-d716-83720203b603"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53 ﻿\n",
            "0 \n",
            "\n",
            "0 \n",
            "\n",
            "1  \n",
            "1  \n",
            "1  \n",
            "1  \n",
            "1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
        "\n",
        "for sequence in sequences.take(2):\n",
        "    print(''.join([int2char[i] for i in sequence.numpy()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1kiIIrfYweT",
        "outputId": "f12e2b84-fd1f-457d-c3bf-5980b2bc8bf4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿\n",
            "\n",
            "     қайтқанда\n",
            "\n",
            "\n",
            "     1\n",
            "\n",
            "үш күндік жолдың бүгінгі соңғы күніне шәкірт бала барын салды\n",
            "қорықтан күн шыға атқа мінейік деп асыққанды бұны қаладан алып қайтқалы барған ағайыны байтасты да таң атаратпа\n",
            "стан өзі оятып тұрғызып еді\n",
            "күнұзын аттан да түспей өзге жүргіншілерден оқ бойы алда отырған кейкейде өзіне таныс көкүйрім мен буратиген тақырбұлақ сияқты қонысқұдықтардың тұстұсына келгенде бала оқшау\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sample(sample):\n",
        "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
        "    for i in range(1, (len(sample)-1) // 2):\n",
        "        input_ = sample[i: i+sequence_length]\n",
        "        target = sample[i+sequence_length]\n",
        "        # extend the dataset with these samples by concatenate() method\n",
        "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
        "        ds = ds.concatenate(other_ds)\n",
        "    return ds\n",
        "\n",
        "dataset = sequences.flat_map(split_sample)"
      ],
      "metadata": {
        "id": "bdu_q6k1ZCHF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_samples(input_, target):\n",
        "    # onehot encode the inputs and the targets\n",
        "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
        "\n",
        "dataset = dataset.map(one_hot_samples)"
      ],
      "metadata": {
        "id": "88XhbMOMZZgN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 2 samples\n",
        "for element in dataset.take(2):\n",
        "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
        "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
        "    print(\"Input shape:\", element[0].shape)\n",
        "    print(\"Target shape:\", element[1].shape)\n",
        "    print(\"=\"*50, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZndzL_lZajP",
        "outputId": "fc5f1639-7f26-44db-aed7-438bc36765e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: ﻿\n",
            "\n",
            "     қайтқанда\n",
            "\n",
            "\n",
            "     1\n",
            "\n",
            "үш күндік жолдың бүгінгі соңғы күніне шәкірт бала барын салды\n",
            "қорықтан к\n",
            "Target: ү\n",
            "Input shape: (100, 54)\n",
            "Target shape: (54,)\n",
            "================================================== \n",
            "\n",
            "Input: \n",
            "\n",
            "     қайтқанда\n",
            "\n",
            "\n",
            "     1\n",
            "\n",
            "үш күндік жолдың бүгінгі соңғы күніне шәкірт бала барын салды\n",
            "қорықтан кү\n",
            "Target: н\n",
            "Input shape: (100, 54)\n",
            "Target shape: (54,)\n",
            "================================================== \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "zl1pelp4ZjLy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.25),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),\n",
        "])"
      ],
      "metadata": {
        "id": "XsBvcdjdZmvF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights_path = f\"results/{BASENAME}-{sequence_length}.h5\"\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQPFmXKc7-Su",
        "outputId": "6e50372b-bde9-45af-d715-82bcbe76c814"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 100, 256)          318464    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 54)                13878     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 857,654\n",
            "Trainable params: 857,654\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make results folder if does not exist yet\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
        "model.save(model_weights_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mdk5VHo99C3",
        "outputId": "7f60ac09-b440-4a06-9bdf-9db7e154ab0e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "5605/5605 [==============================] - 187s 32ms/step - loss: 1.8853 - accuracy: 0.4065\n",
            "Epoch 2/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.5553 - accuracy: 0.5026\n",
            "Epoch 3/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 1.4470 - accuracy: 0.5354\n",
            "Epoch 4/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.3749 - accuracy: 0.5558\n",
            "Epoch 5/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 1.3194 - accuracy: 0.5721\n",
            "Epoch 6/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.2738 - accuracy: 0.5859\n",
            "Epoch 7/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.2354 - accuracy: 0.5967\n",
            "Epoch 8/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.2027 - accuracy: 0.6067\n",
            "Epoch 9/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 1.1748 - accuracy: 0.6151\n",
            "Epoch 10/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 1.1492 - accuracy: 0.6226\n",
            "Epoch 11/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 1.1285 - accuracy: 0.6289\n",
            "Epoch 12/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.1103 - accuracy: 0.6346\n",
            "Epoch 13/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.0935 - accuracy: 0.6394\n",
            "Epoch 14/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.0790 - accuracy: 0.6435\n",
            "Epoch 15/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 1.0658 - accuracy: 0.6478\n",
            "Epoch 16/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.0548 - accuracy: 0.6514\n",
            "Epoch 17/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.0421 - accuracy: 0.6548\n",
            "Epoch 18/30\n",
            "5605/5605 [==============================] - 182s 33ms/step - loss: 1.0323 - accuracy: 0.6577\n",
            "Epoch 19/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.0226 - accuracy: 0.6607\n",
            "Epoch 20/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.0128 - accuracy: 0.6639\n",
            "Epoch 21/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 1.0049 - accuracy: 0.6655\n",
            "Epoch 22/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 0.9958 - accuracy: 0.6692\n",
            "Epoch 23/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 1.0149 - accuracy: 0.6637\n",
            "Epoch 24/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 0.9744 - accuracy: 0.6755\n",
            "Epoch 25/30\n",
            "5605/5605 [==============================] - 183s 33ms/step - loss: 0.9770 - accuracy: 0.6748\n",
            "Epoch 26/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 0.9692 - accuracy: 0.6766\n",
            "Epoch 27/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 0.9617 - accuracy: 0.6791\n",
            "Epoch 28/30\n",
            "5605/5605 [==============================] - 185s 33ms/step - loss: 0.9575 - accuracy: 0.6805\n",
            "Epoch 29/30\n",
            "5605/5605 [==============================] - 185s 33ms/step - loss: 0.9484 - accuracy: 0.6830\n",
            "Epoch 30/30\n",
            "5605/5605 [==============================] - 184s 33ms/step - loss: 0.9423 - accuracy: 0.6851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Text**"
      ],
      "metadata": {
        "id": "U8VsS5z8PkuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation\n",
        "import os\n",
        "\n",
        "sequence_length = 100\n",
        "# dataset file path\n",
        "FILE_PATH = \"data/1-tom.txt\"\n",
        "BASENAME = os.path.basename(FILE_PATH)\n",
        "\n",
        "seed = \"алыста келе жатқан жолаушыны көріп\"\n",
        "\n",
        "# load vocab dictionaries\n",
        "char2int = pickle.load(open(f\"{BASENAME}-char2int.pickle\", \"rb\"))\n",
        "int2char = pickle.load(open(f\"{BASENAME}-int2char.pickle\", \"rb\"))\n",
        "vocab_size = len(char2int)\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
        "    Dropout(0.25),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.load_weights(f\"results/{BASENAME}-{sequence_length}.h5\")\n",
        "\n",
        "s = seed\n",
        "n_chars = 400\n",
        "generated = \"\"\n",
        "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
        "    # make the input sequence\n",
        "    X = np.zeros((1, sequence_length, vocab_size))\n",
        "    for t, char in enumerate(seed):\n",
        "      X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
        "    # predict the next character\n",
        "    predicted = model.predict(X, verbose=0)[0]\n",
        "    # converting the vector to an integer\n",
        "    next_index = np.argmax(predicted)\n",
        "    # converting the integer to a character\n",
        "    next_char = int2char[next_index]\n",
        "    # add the character to results\n",
        "    generated += next_char\n",
        "    # shift seed and the predicted character\n",
        "    seed = seed[1:] + next_char\n",
        "\n",
        "print(\"Seed:\", s)\n",
        "print(\"Generated text:\")\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "IbpqwmCe8BP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19f1669-c315-47b1-98bb-afcd6eeb373b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating text: 100%|██████████| 400/400 [00:23<00:00, 17.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: алыста келе жатқан жолаушыны көріп\n",
            "Generated text:\n",
            " екеуінің де бірі бала да алшынбай бұл ауылда бар атшабарлар абайдың жаңағы ашуы бар басты байсалды байқаса қарап алып бұйрық етті\n",
            "жақып таныс алған жоқ\n",
            "бірақ құнанбай аулынан қарап тұрып барып абай байдалының арасынан басқан екен абай жаңағы «тобықтығы» деген сөздер болатын\n",
            "осы кезде абай жаңағы «бұл  деді\n",
            "содан бері бұға бұған соқ теңіздеп айтқан жақында абайға қарап алып еді үй іші тегіс жатыр \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}